import time
import random

import numpy as np
import tensorflow as tf
from tensorflow.python import pywrap_tensorflow


from ReplayBuffer import ReplayBuffer
from ReplayBuffer import Transition
from ReplayBuffer import Episode

from RunningMeanStd import RunningMeanStd
from environment_wrapper import environment
from Configurations import Configurations
from Utils import Plot

# os.environ['CUDA_VISIBLE_DEVICES'] = ''

class Policy:
	def __init__(self, sess, action_size):
		self._sess = sess
		self._scope = "policy"

		self._mean, self._logstd = self.createModel(action_size) 
		self._std = tf.exp(self._logstd)

		self._action = self._mean + self._std*tf.random_normal(tf.shape(self._mean))
		self._neglogprob = self.neglogprob(self._action)


	def neglogprob(self, x):
		return 0.5 * tf.reduce_sum(tf.square((x - self._mean) / self._std), axis=-1) \
			+ 0.5 * np.log(2.0 * np.pi) * tf.to_float(tf.shape(x)[-1]) \
			+ tf.reduce_sum(self._logstd, axis=-1)

	def createModel(self, action_size):
		with tf.variable_scope(self._scope, reuse=False):
			layer = self._state
			for i in range(Configurations.instance().policyLayerNumber):
				layer = tf.keras.layers.Dense(
					layer, 
					Configurations.instance().policyLayerSize,
					name = "L{}".format(i),
					activation=Configurations.instance().activationFunction,
					kernel_initializer=Configurations.instance().kernelInitializationFunction
				)

			mean = tf.keras.layers.Dense(
				layer, 
				action_size,
				name = "mean",
				kernel_initializer=Configurations.instance().kernelInitializationFunction
			)

			logstd = tf.Variable(
				initial_value = np.zeros(action_size),
				trainable=True,
				name='logstd'
			)
		return mean, logstd


	def getAction(self, states):
		action, neglogprob = self._sess.run(
			[self._action, self._neglogprob],
			feed_dict={self._state:states}
		)
		return action, neglogprob

	def getMeanAction(self, states):
		action = self._sess.run(
			self._mean,
			feed_dict={self._state:states}
		)
		return action

	@property
	def std(self):
		return self._std
	

class ValueFunction:
	def __init__(self, sess, state):
		self._sess = sess
		self._state = state
		self._scope = "valueFunction"
		self._value = self.createNetwork()

	def createNetwork(self):
		with tf.variable_scope(self._scope, reuse=False):
			layer = self._state
			for i in range(Configurations.instance().valueLayerSize):
				layer = tf.keras.layers.Dense(
					layer, 
					Configurations.instance().valueLayerNumber,
					name = "L{}".format(i),
					activation=Configurations.instance().activationFunction,
					kernel_initializer=Configurations.instance().kernelInitializationFunction
				)

			value = tf.keras.layers.Dense(
				layer, 
				1,
				name = "value",
				kernel_initializer=Configurations.instance().kernelInitializationFunction
			)
		return value


	def getValue(self, states):
		value = self._sess.run(
			self._value,
			feed_dict={self._state:states}
		)
		return value

	@property
	def value(self):
		return self._value
	

class TrackingController:
	def __init__(self):
		random.seed(int(time.time()))
		np.random.seed(int(time.time()))
		tf.set_random_seed(int(time.time()))

		self._startTime = time.time()
		self._simulationTime = 0
		self._trainingTime = 0

	def initialize(self, configuration_filepath=""):
		self._sess = tf.Session()

		self._configurationFilePath = configuration_filepath
		Configurations.instance().loadData(configuration_filepath)

		# get parameters from config
		self._numSlaves 				= Configurations.instance().numSlaves
		self._motion 					= Configurations.instance().motion

		self._gamma 					= Configurations.instance().gamma
		self._lambd 					= Configurations.instance().lambd
		self._clipRange					= Configurations.instance().clipRange

		self._learningRatePolicy 		= Configurations.instance().learningRatePolicy
		self._learningRatePolicyDecay	= Configurations.instance().learningRatePolicyDecay
		self._learningRateValueFunction = Configurations.instance().learningRateValueFunction		

		self._batchSize 				= Configurations.instance().batchSize
		self._transitionsPerIteration 	= Configurations.instance().transitionsPerIteration

		self._trajectoryLength          = Configurations.instance().trajectoryLength
		self._useOrigin					= Configurations.instance().useOrigin
		self._originOffset				= Configurations.instance().originOffset

		self._adaptiveSamplingSize		= Configurations.instance().adaptiveSamplingSize

		# if useEvaluation is true, evaluation of training progress is performed by evaluation function, else it is done by transitions collected in training session.
		self._useEvaluation 			= Configurations.instance().useEvaluation

		self._sessionName				= Configurations.instance().sessionName

		# initialize environment
		self._env = environment()
		self._stateSize = self._env.getStateSize()
		self._actionSize = self._env.getActionSize()
		self._rms = RunningMeanStd(shape=(self._stateSize))

		self._state = tf.placeholder(tf.float32, shape=[None,self._stateSize], name='state')

		# initialize networks
		self._policy = Policy(self._sess, self._state, self._actionSize)
		self._valueFunction = ValueFunction(self._sess, self._state)

		# initialize RunningMeanStd
		self._rms = RunningMeanStd(shape=(self._stateSize))

		# initialize replay buffer
		self._replayBuffer = ReplayBuffer()


		# build training operation
		self.buildTrainOp()

		# initialize motion generator
		self._motionGenerator = RNNManager(1, self._motion)
		self._timeIndices = [None]*self.num_slaves

		# initialize adaptive sampler
		self._adaptiveSampler = AdaptiveSampler(self._adaptiveSamplingSize)

		self._sess.run(tf.global_variables_initializer())

		# initialize saver
		self._saver = tf.train.Saver(var_list=tf.trainable_variables(), max_to_keep=1)
		# save maximum step network
		self._smax = 0
		# save maximum reward network
		self._rmax = 0


		# initialize statistics variables
		# TODO
		self._summary_num_log = 0
		self._summary_num_episodes_total = 0
		self._summary_num_transitions_total = 0

		self._summary_max_episode_length = 0

		self._summary_total_rewards = []
		self._summary_total_rewards_by_parts = np.array([[]]*5)
		self._summary_mean_rewards = []
		self._summary_transition_per_episodes = []
		self._summary_noise_records = []

		self._summary_evaluation_total_rewards = []
		self._summary_evaluation_total_rewards_by_parts = np.array([[]]*5)
		self._summary_evaluation_mean_rewards = []
		self._summary_evaluation_transition_per_episodes = []

		self._isNetworkLoaded = False
		self._loadedNetwork = ""


	# load trained networks & rms
	def loadNetowrks(self, directory, network_num):
		# load rms
		rms_dir = "{}/rms/".format(directory)
		mean_dir = rms_dir+"mean_{}.npy".format(network_num)
		var_dir = rms_dir+"var_{}.npy".format(network_num)
		if os.path.exists(mean_dir):
			print("Loading RMS parameters")
			self.RMS.mean = np.load(mean_dir)
			self.RMS.var = np.load(var_dir)
			self.RMS.count = 200000000

		# load netowrk
		network_dir = "{}/network-{}".format(directory, network_num)
		print("Loading networks from {}".format(network_dir))

		def get_tensors_in_checkpoint_file(file_name):
			varlist=[]
			var_value =[]
			reader = pywrap_tensorflow.NewCheckpointReader(file_name)
			var_to_shape_map = reader.get_variable_to_shape_map()
			for key in sorted(var_to_shape_map):
				varlist.append(key)
				var_value.append(reader.get_tensor(key))
			return (varlist, var_value)


		saved_variables, saved_values = get_tensors_in_checkpoint_file(network_dir)
		saved_dict = {n : v for n, v in zip(saved_variables, saved_values)}
		restore_op = []
		for v in tf.trainable_variables():
			if v.name[:5] != "policy" and v.name[:6] != "valueFunction":
				continue
			if v.name[:-2] in saved_dict:
				saved_v = saved_dict[v.name[:-2]]
				if v.shape == saved_v.shape:
					print("   Restoring {}".format(v.name[:-2]))
					restore_op.append(v.assign(saved_v))

		restore_op = tf.group(*restore_op)
		self._sess.run(restore_op)

		self._isNetworkLoaded = True
		self._loadedNetwork = "{} - {}".format(directory, network_num)

	def generateTrajectory(self):
		if self._useOrigin:
			self._trajectory, self._goalTrajectory = self.rnn_manager.getOriginalTrajectory(self._trajectoryLength, self._originOffset)
		else:
			traj_filename = "../trajectories/{}/traj.npy".format(self._motion)
			goal_filename = "../trajectories/{}/goal.npy".format(self._motion)
			if os.path.exists(traj_filename) and os.path.exists(goal_filename):
				print("Loading trajectories from {}".format(traj_filename))
				traj = np.load(traj_filename)
				goal_traj = np.load(goal_filename)
				if traj.shape[1] < self._trajectoryLength:
					print("motion is too short, required : {}, maximum : {}".format(self._trajectoryLength, traj.shape[1]))
					# exit()
					self._trajectoryLength = traj.shape[1]
				self._trajectory = traj[0][:self._trajectoryLength]
				self._goalTrajectory = goal_traj[0][:self._trajectoryLength]
			else:
				traj, goal_traj = self.rnn_manager.getTrajectory(self._trajectoryLength)
				self._trajectory = traj[0]
				self._goalTrajectory = goal_traj[0]

	# build training operations for policy and value function
	def buildTrainOp(self):
		with tf.variable_scope('Optimize'):
			self._action = tf.placeholder(tf.float32, shape=[None,self._actionSize], name='action')
			self._TD = tf.placeholder(tf.float32, shape=[None], name='TD')
			self._GAE = tf.placeholder(tf.float32, shape=[None], name='GAE')
			self._oldNeglogprobs = tf.placeholder(tf.float32, shape=[None], name='oldNeglogprobs')
			self._decayedLearningRatePolicy = tf.placeholder(tf.float32, shape=[], name='decayedLearningRatePolicy')

			self._curNeglogp = self._policy.neglogp(self._action)
			self._ratio = tf.exp(self._oldNeglogprobs-self._curNeglogp)
			clipped_ratio = tf.clip_by_value(self._ratio, 1.0 - self._clipRange, 1.0 + self._clipRange)

			surrogate = -tf.reduce_mean(tf.minimum(self._ratio*self._GAE, clipped_ratio*self._GAE))
			valueLoss = tf.reduce_mean(tf.square(self._valueFunction.value - self._TD))

		policyTrainer = tf.train.AdamOptimizer(learning_rate=self._decayedLearningRatePolicy)
		grads, params = zip(*policyTrainer.compute_gradients(surrogate));
		grads, _ = tf.clip_by_global_norm(grads, 0.5)
		
		gradsAndVars = list(zip(grads, params))
		self._policyTrainOp = policyTrainer.apply_gradients(gradsAndVars)


		valueFunctionTrainer = tf.train.AdamOptimizer(learning_rate=self._learningRateValueFunction)
		grads, params = zip(*valueFunctionTrainer.compute_gradients(valueLoss));
		grads, _ = tf.clip_by_global_norm(grads, 0.5)
		
		gradsAndVars = list(zip(grads, params))
		self._valueFuntionTrainOp = valueFunctionTrainer.apply_gradients(gradsAndVars)


	def computeTDAndGAE(self):
		self._replayBuffer.clear()
		for epi in self._collectedEpisodes:
			data = epi.data
			size = len(data)

			# update max episorde length
			if size > self._summary_max_episode_length:
				self._summary_max_episode_length = size

			states, actions, rewards, values, neglogprobs, TDs, GAEs = zip(*data)
			values = np.concatenate((values, [0]), axis=0)
			advantages = np.zeros(size)
			ad_t = 0

			for i in reversed(range(size)):
				delta = rewards[i] + values[i+1] * self.gamma - values[i]
				ad_t = delta + self.gamma * self.lambd * ad_t
				advantages[i] = ad_t

			TD = values[:size] + advantages
			for i in range(size):
				self._replayBuffer.push(states[i], actions[i], rewards[i], values[i], neglogprobs[i], TD[i], advantages[i])


	def optimize(self):
		self.computeTDandGAE()
		if len(self._replayBuffer.buffer) < self._batchSize:
			return

		transitions = np.array(self._replayBuffer.buffer)
		GAE = np.array(Transition(*zip(*transitions)).GAE)
		GAE = (GAE - GAE.mean())/(GAE.std() + 1e-5)

		ind = np.arange(len(GAE))

		np.random.shuffle(ind)

		for s in range(int(len(ind)//self._batchSize)):
			selectedIndex = ind[s*self._batchSize:(s+1)*self._batchSize]
			selectedTransitions = transitions[selectedIndex]

			batch = Transition(*zip(*selectedTransitions))

			# GAE = np.array(batch.GAE)
			# GAE = (GAE - GAE.mean())/(GAE.std() + 1e-5)
			self._sess.run([self._policyTrainOp, self._valueFuntionTrainOp], 
				feed_dict={
					self._state:batch.s, 
					self._TD:batch.TD, 
					self._action:batch.a, 
					self._oldNeglogprobs:batch.neglogprob, 
					self._GAE:GAE[selectedIndex],
					self._decayedLearningRatePolicy:self._learningRatePolicy
				}
			)


	def reset(self):
		return

	def runTraining(self, num_iteration=1):
		# create logging directory
		self._directory = self._sessionName

		if not os.path.exists("../output/"):
			os.mkdir("../output/")
		self._directory = '../output/'+self._sessionName+'/'

		if not os.path.exists(self._directory):
			os.mkdir(self._directory)
		directory = self._directory + "rms/"
		if not os.path.exists(directory):
			os.mkdir(directory)
		directory = directory + "cur/"
		if not os.path.exists(directory):
			os.mkdir(directory)

		self.printParameters()

		while True:
			print("\nTraining start")
			self._summary_num_episodes_per_iteration = 0
			self._summary_num_transitions_per_iteration = 0
			self._summary_reward_per_iteration = 0
			self._summary_reward_by_part_per_iteration = []
			self._summary_max_episode_length = 0

			for it in range(num_iteration):
				self._summary_sim_time -= time.time()
				self._collectedEpisodes = []

				nan_count = 0

				for i in range(self.num):
					# select time indices
					index, timet = self._adaptiveSampler.selectTime()
					self._timeIndices[i] = index
					self._env.reset(i, timet)

				# get new states
				states = self._env.getStates()
				states = self._rms.apply(states)

				actions = [None]*self._numSlaves
				rewards = [None]*self._numSlaves
				episodes = [None]*self._numSlaves

				terminated = [False]*self._numSlaves

				for j in range(self._numSlaves):
					episodes[j] = Episode()

				local_step = 0
				last_print = 0
				while True:
					# set action
					actions, logprobs = self._policy.getAction(states)
					values = self._valueFunction.getValue(states)
					self._env.setActions(actions)

					# run one step
					self._env.steps()

					for j in range(self._numSlaves):
						if terminated[j]:
							continue

						is_terminal, nan_occur, end_of_trajectory = self._env.isNanAtTerminal(j)
						# push tuples only if nan did not occur
						if nan_occur is not True:
							r = self._env.getReward(j)
							rewards[j] = r[0]
							self._summary_reward_per_iteration += rewards[j]
							self._summary_reward_by_part_per_iteration.append(r)
							episodes[j].Push(states[j], actions[j], rewards[j], values[j], logprobs[j])
							local_step += 1
						else:
							nan_count += 1

						# if episode is terminated
						if is_terminal:
							# push episodes
							if len(episodes[j].GetData()) != 0:
								self._collectedEpisodes.append(episodes[j])

								# update adaptive sampling weights
								self._adaptiveSampler.updateWeights(self._timeIndices[j], self.GetEpiReward(episodes[j]), end_of_trajectory)

							if local_step < self._transitionsPerIteration:
								episodes[j] = Episode()

								# select time index
								index, timet = self._adaptiveSampler.selectTime()
								self._timeIndices[j] = index
								
								self._env.reset(j, timet) 
							else:
								terminated[j] = True

					# if local step exceeds t_p_i: wait for others to terminate
					if local_step >= self._transitionsPerIteration:  
						if all(t is True for t in terminated):
							print('{}/{} : {}/{}'.format(it+1, num_iteration, local_step, self._transitionsPerIteration),end='\r')
							break

					# print progress per 100 steps
					if last_print + 100 < local_step: 
						print('{}/{} : {}/{}'.format(it+1, num_iteration, local_step, self._transitionsPerIteration),end='\r')
						last_print = local_step

					# update states				
					states = self.Env.getStates()
					states_for_update = states[~np.array(terminated)]  
					states_for_update = self.RMS.apply(states_for_update)
					states[~np.array(terminated)] = states_for_update

				self._summary_sim_time += time.time()
				self._summary_train_time -= time.time()

				# optimization
				print('')
				if(nan_count > 0):
					print("nan_count : {}".format(nan_count))
				self.Optimize()  ##SM) after getting all tuples, optimize once
				self._summary_num_episodes_per_iteration += len(self._collectedEpisodes)
				self._summary_num_transitions_per_iteration += local_step

				self._summary_train_time += time.time()


			# decay learning rate
			if self._learningRatePolicy > 1e-5:
				self._learningRatePolicy = self._learningRatePolicy * self._learningRatePolicyDecay



			print('Training end\n')
			if(self._useEvaluation):
				self.evaluation()


			self._summary_total_rewards.append(self._summary_reward_per_iteration/self._summary_num_episodes_per_iteration)
			self._summary_total_rewards_by_parts = np.insert(self._summary_total_rewards_by_parts, self._summary_total_rewards_by_parts.shape[1], np.asarray(self._summary_reward_by_part_per_iteration).sum(axis=0)/self._summary_num_episodes_per_iteration, axis=1)
			self._summary_mean_rewards.append(np.asarray(self._summary_total_rewards)[-10:].mean())
			self._summary_noise_records.append(self._sess.run(self._policy.std).mean())


			self._summary_num_episodes_total += self._summary_num_episodes_per_iteration
			self._summary_num_transitions_total += self._summary_num_transitions_per_iteration
			t_per_e = 0
			if self._summary_num_episodes_per_iteration is not 0:
				t_per_e = self._summary_num_transitions_per_iteration / self._summary_num_episodes_per_iteration
			self._summary_transition_per_episodes.append(t_per_e)

			# print summary
			self.printSummary()


	def evaluation(self):
		return

	def play(self):
		# create logging directory
		self._directory = self._sessionName

		if not os.path.exists("../output/"):
			os.mkdir("../output/")
		self._directory = '../output/'+self._sessionName+'_play/'

		if not os.path.exists(self._directory):
			os.mkdir(self._directory)
		directory = self._directory + "rms/"
		if not os.path.exists(directory):
			os.mkdir(directory)
		directory = directory + "cur/"
		if not os.path.exists(directory):
			os.mkdir(directory)

		self.printParameters()
		return

	def printParameters(self):
		# print on shell
		print("===============================================================")
		print(datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
		print("Elapsed time         : {:.2f}s".format(time.time()-self._startTime))
		print("Session Name         : {}".format(self._sessionName))
		print("Motion               : {}".format(self._motion))
		print("Slaves number        : {}".format(self._numSlaves))
		print("State size           : {}".format(self._stateSize))
		print("Action size          : {}".format(self._actionSize))
		print("Learning rate        : {:.6f}".format(self._learningRatePolicy))
		print("Gamma                : {}".format(self._gamma))
		print("Lambda               : {}".format(self._lambd))
		print("Batch size           : {}".format(self._batchSize))
		print("Transitions per iter : {}".format(self._transitionsPerIteration))
		print("PPO clip range       : {}".format(self._clipRange))
		print("Trajectory length    : {}".format(self._trajectoryLength))
		print("Loaded netowrks      : {}".format(self._loadedNetwork))
		print("===============================================================")

		# print to file
		out = open(self._directory+"parameters", "w")
		out.write(datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S\n"))
		out.write("Session Name         : {}\n".format(self._sessionName))
		out.write("Motion               : {}\n".format(self._motion))
		out.write("Slaves number        : {}\n".format(self._numSlaves))
		out.write("State size           : {}\n".format(self._stateSize))
		out.write("Action size          : {}\n".format(self._actionSize))
		out.write("Learning rate        : {:.6f}\n".format(self._learningRatePolicy))
		out.write("Gamma                : {}\n".format(self._gamma))
		out.write("Lambda               : {}\n".format(self._lambd))
		out.write("Batch size           : {}\n".format(self._batchSize))
		out.write("Transitions per iter : {}\n".format(self._transitionsPerIteration))
		out.write("PPO clip range       : {}\n".format(self._clipRange))
		out.write("Trajectory length    : {}\n".format(self._trajectoryLength))
		out.write("Loaded netowrks      : {}\n".format(self._loadedNetwork))
		out.close()

		# pre make results file
		out = open(self._directory+"results", "w")
		out.close()

		# copy configuration file
		cmd = "cp {} {}".format(self._configurationFilePath, self._directory)
		os.system(cmd)

		return


	def printSummary(self):
		np.save(self._directory+"rms/mean_{}.npy".format(self._summary_num_log),self._rms.mean)
		np.save(self._directory+"rms/var_{}.npy".format(self._summary_num_log),self._rms.var)

		print('===============================================================')
		print(datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
		print("Elapsed time         : {:.2f}s".format(time.time()-self._startTime))
		print("Simulation time      : {}s".format(self._summary_sim_time))
		print("Training time        : {}s".format(self._summary_train_time))
		print("Session Name         : {}".format(self._sessionName))
		print("Logging Count        : {}".format(self._summary_num_log))
		print('Noise                : {:.3f}'.format(self._summary_noise_records[-1]))
		print('Learning rate        : {:.6f}'.format(self._learningRatePolicy))
		print('Total episode        : {}'.format(self._summary_num_episodes_total))
		print('Total trans          : {}'.format(self._summary_num_transitions_total))
		total_t_per_e = 0
		if self._summary_num_episodes_total is not 0:
			total_t_per_e = self._summary_num_transitions_total / self._summary_num_episodes_total
		print('Total trans per epi  : {:.2f}'.format(total_t_per_e))
		print('Episode              : {}'.format(self._summary_num_episodes_per_iteration))
		print('Transition           : {}'.format(self._summary_num_transitions_per_iteration))
		print('Trans per epi        : {:.2f}'.format(self._summary_transition_per_episodes[-1]))
		print('Max episode length   : {}'.format(self._summary_max_episode_length))
		print('Rewards per episodes : {:.2f}'.format(self._summary_total_rewards[-1]))

		if(self.use_evaluation):
			evaluation_t_per_e = self._summary_evaluation_num_transitions_per_iteration/self._numSlaves
			self._summary_evaluation_transition_per_episodes.append(evaluation_t_per_e)
			print('Eval trans per epi   : {:.2f}'.format(evaluation_t_per_e))
			print('Eval rew per epi     : {:.2f}'.format(self._summary_evaluation_total_rewards[-1]))
		print('===============================================================')


		# print plot
		if(self.use_evaluation):
			y_list = [[np.asarray(self._summary_evaluation_total_rewards_by_parts[0]), 'r'], 
						[np.asarray(self._summary_evaluation_mean_rewards), 'r_mean'],
						[np.asarray(self._summary_evaluation_transition_per_episodes), 'steps'], 
						[np.asarray(self._summary_evaluation_total_rewards_by_parts[1]), 'p'], 
						[np.asarray(self._summary_evaluation_total_rewards_by_parts[2]), 'v'], 
						[np.asarray(self._summary_evaluation_total_rewards_by_parts[3]), 'com'],
						[np.asarray(self._summary_evaluation_total_rewards_by_parts[4]), 'ee']]
			Plot(y_list,self._sessionName,1,path=self._directory+"result.png")

			for i in range(len(y_list)):
				y_list[i][0] = np.array(y_list[i][0])/np.array(self._summary_evaluation_transition_per_episodes)
			y_list[1][0] = np.asarray(self._summary_noise_records)
			y_list[1][1] = 'noise'

			Plot(y_list,self._sessionName+"_per_step",2,path=self._directory+"result_per_step.png")
		else:
			y_list = [[np.asarray(self._summary_total_rewards_by_parts[0]), 'r'], 
						[np.asarray(self._summary_mean_rewards), 'r_mean'],
						[np.asarray(self._summary_transition_per_episodes), 'steps'], 
						[np.asarray(self._summary_total_rewards_by_parts[1]), 'p'], 
						[np.asarray(self._summary_total_rewards_by_parts[2]), 'v'], 
						[np.asarray(self._summary_total_rewards_by_parts[3]), 'com'],
						[np.asarray(self._summary_total_rewards_by_parts[4]), 'ee']]
			Plot(y_list,self._sessionName,1,path=self._directory+"result.png")

			for i in range(len(y_list)):
				y_list[i][0] = np.array(y_list[i][0])/np.array(self._summary_transition_per_episodes)
			y_list[1][0] = np.asarray(self._summary_noise_records)
			y_list[1][1] = 'noise'

			Plot(y_list,self._sessionName+"_per_step",2,path=self._directory+"result_per_step.png")


		# log to file
		out = open(self._directory+"results", "a")
		out.write('===============================================================\n')
		out.write(datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S\n"))
		out.write("Elapsed time         : {:.2f}s\n".format(time.time()-self._startTime))
		out.write("Simulation time      : {}s\n".format(self._summary_sim_time))
		out.write("Training time        : {}s\n".format(self._summary_train_time))
		out.write("Session Name         : {}\n".format(self._sessionName))
		out.write("Logging Count        : {}\n".format(self._summary_num_log))
		out.write('Noise                : {:.3f}\n'.format(self._summary_noise_records[-1]))
		out.write('Learning rate        : {:.6f}\n'.format(self._learningRatePolicy))
		out.write('Total episode        : {}\n'.format(self._summary_num_episodes_total))
		out.write('Total trans          : {}\n'.format(self._summary_num_transitions_total))
		out.write('Total trans per epi  : {:.2f}\n'.format(total_t_per_e))
		out.write('Episode              : {}\n'.format(self._summary_num_episodes_per_iteration))
		out.write('Transition           : {}\n'.format(self._summary_num_transitions_per_iteration))
		out.write('Trans per epi        : {:.2f}\n'.format(self._summary_transition_per_episodes[-1]))
		out.write('Max episode length   : {}\n'.format(self._summary_max_episode_length))
		out.write('Rewards per episodes : {:.2f}\n'.format(self._summary_total_rewards[-1]))

		if(self.use_evaluation):
			evaluation_t_per_e = self._summary_evaluation_num_transitions_per_iteration/self._numSlaves
			self._summary_evaluation_transition_per_episodes.append(evaluation_t_per_e)
			out.write('Eval trans per epi   : {:.2f}\n'.format(evaluation_t_per_e))
			out.write('Eval rew per epi     : {:.2f}\n'.format(self._summary_evaluation_total_rewards[-1]))
		out.write('===============================================================\n')
		out.close()


		# save reward
		self.trajectory_manager.saveTimeWeight(self._directory, self._summary_num_log)


		# save network
		self.Save(self._directory+"network")

		if t_per_e > self._smax:
			self._smax = t_per_e
			np.save(self._directory+"rms/mean_smax.npy",self.RMS.mean)
			np.save(self._directory+"rms/var_smax.npy",self.RMS.var)

			os.system("cp {}/network-{}.data-00000-of-00001 {}/network-smax.data-00000-of-00001".format(self._directory, 0, self._directory))
			os.system("cp {}/network-{}.index {}/network-smax.index".format(self._directory, 0, self._directory))
			os.system("cp {}/network-{}.meta {}/network-smax.meta".format(self._directory, 0, self._directory))


		if self._useEvaluation:
			tr = self._summary_evaluation_total_rewards[-1]
		else:
			tr = self._summary_total_rewards[-1]

		if tr > self._rmax:
			self._rmax = tr
			np.save(self._directory+"rms/mean_rmax.npy",self._rms.mean)
			np.save(self._directory+"rms/var_rmax.npy",self._rms.var)

			os.system("cp {}/network-{}.data-00000-of-00001 {}/network-rmax.data-00000-of-00001".format(self._directory, 0, self._directory))
			os.system("cp {}/network-{}.index {}/network-rmax.index".format(self._directory, 0, self._directory))
			os.system("cp {}/network-{}.meta {}/network-rmax.meta".format(self._directory, 0, self._directory))

		self._summary_num_log = self._summary_num_log + 1

		return